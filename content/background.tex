\section{Background}

This section contains background information on which our approach is based.

\subsection{Word Embeddings}

In natural language processing, word embeddings are used to represent the semantic meaning of words. A word embedding is an $m$-dimensional vector that gives a distributed representation of a word with each vector dimension representing some semantic feature of the word. Words with similar semantic meaning will be clustered together in the vector space since similar words will be used in similar context~\cite{mikolov2013distributed}.

GloVe (Gloval Vectors for word representation)~\cite{pennington2014glove} and Skip-gram~\cite{mikolov2013efficient} are the two of the most popular word embedding learning models. Both operate on the same fundamental idea that the basic semantic meaning of a word can be represented by utilizing surrounding words as semantic context. GloVe uses a co-occurance matrix of all the words in a corpus. When the matrix has been filled with all word co-occurances, matrix factorization is performed to produce the vector values of each word's embedding. Skip-gram defines a window size that indicates the number of surrounding words to use as context. A given word's vector values are updated by using its context as input to a neural network.

After a corpus has been input to either of these algorithms and all words have been processed, the resulting word embeddings should have grouped similar words together in the vector space. That is, the closer two words are to each other, the more similar their semantic meaning is. For our purposes, we will use the popular Skip-gram algorithm Word2Vec. We propose to use a modified version of Word2Vec to obtain word embeddings for words in source code. 
 
\subsection{Recurrent Neural Networks}

Recurrent Neural Networks (RNNs) are neural networks that process sequences of information in steps~\cite{elman1990finding}. For each step in an RNN, new input is processed with the input that came before it. That is, for step $t$, the new input $x_t$ is passed to the hidden state, $h_t$, of step $t$. $h_t$ then uses $x_t$ and the output of the previous step's hidden state $h_{t-1}$ to calculate the current output using the function $Vx_t + Wh_{t-1}$. The result is then usually transformed using a nonlinear function $f$, such as $tanh$. So, the final calculation for the hidden layer (and the output of that layer $r_t$) is $r_t = h_t = f(Vx_t + Wh_{t-1})$. The hidden state of the current step is then used in the next step's calculation with the next input, $x_{t+1}$. The steps continue until all input has been consumed, at which time the final output is produced. The final output can be used in a number of ways. For example, for a classification problem the output is usually transformed by a softmax classifier to determine the classification of the input. During training, the classification by the RNN is matched against the ground truth. If the RNN classification is incorrect, a loss is determined which is then used during backpropagation to update the RNN weights, which should result in better classifications by the RNN in the future.

\subsection{Related Work}

To our knowledge, we are the first to use deep learning techniques to analyze source code. There have been many applications of deep learning techniques used in natural language processing~\cite{collobert2008unified}~\cite{glorot2011domain}. The popularity of applying these techniques to analyze natural language has grown rapidly in recent years. Before the use of deep learning, the main focus of analysis of natural language was the use of statistical language models. Hindle et al~\cite{hindle2012naturalness} showed that the same statistical language models used to analyze natural language were just as useful (if not more so) in analyzing code in software. Since analysis of natural language has made a shift to using deep learning techniques from statistical language modeling, it is worth exploring the application of deep learning on software.

Statistical machine translation (SMT) is based in statical language models and has been widely used to analyze and translate source code. Nguyen et al~\cite{nguyen2013lexical} conducted experiments that attempted to translate Java code to C\# code using SMT. Source code was treated as a simple sequence of tokens and was translated on a method-by-method basis. The results showed a need for improvement, but indicated potential. More recently, Vasilescu et al~\cite{vasilescu2017recovering} described a tool \textit{Autonym} which used SMT to recover orignal names from minified names in JavaScript programs. The results were comperable to other state of the art deobfuscation tools, however the names \textit{Autonym} obtained easily were difficult for other tools to obtain. Conversly, the names \textit{Autonym} had trouble obtaining were easily identified by other tools. This lead the authors to blend their tool with another, \textit{JSnice}~\cite{raychev2015predicting}, which lead to a significant improvement in results.
